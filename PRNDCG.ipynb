{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking and Recall Data Analysis\n",
    "\n",
    "This notebook is dedicated to the analysis of ranking and recall data related to the MIRCV project. It examines the performance of the developed ranking system using data extracted from text files. The following custom functions are used to read, process, and prepare the data for detailed analysis.\n",
    "\n",
    "## Notebook Structure\n",
    "- **Reading Functions**:  Define the method for reading data from text files and organizing them into useful data structures.\n",
    "- **Data Loading**: Loads ranking and recall data from specific files for analysis.\n",
    "- **Data Access and Analysis**: Displays and analyzes specific parts of the data to gain initial insights into the performance of the ranking system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Functions\n",
    "\n",
    "# read_data: Reads ranking data from a file and organizes it into a dictionary\n",
    "def read_data(file_path):\n",
    "    \"\"\"Reads ranking data from a file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path of the file to read.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing the ranking data.\n",
    "    \"\"\"\n",
    "    system_ranking = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            qid, doc_id, relevance = parts[0], str(int(parts[2])), int(parts[3])\n",
    "            if qid not in system_ranking:\n",
    "                system_ranking[qid] = []\n",
    "            system_ranking[qid].append((doc_id, relevance))\n",
    "        \n",
    "    return system_ranking\n",
    "\n",
    "# read_data_recall_base: Reads base recall data from a file and organizes it into a dictionary\n",
    "def read_data_recall_base(file_path):\n",
    "    \"\"\"Reads base recall data from a file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path of the file to read.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing the base recall data.\n",
    "    \"\"\"\n",
    "    recalls = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            qid, base = parts[0], int(parts[1])\n",
    "            if qid not in recalls:\n",
    "                recalls[qid] = {}\n",
    "            recalls[qid] = base\n",
    "\n",
    "    return recalls\n",
    "\n",
    "# read_data: Reads ranking data from a file and organizes it into a dictionary\n",
    "def read_data_2(file_path):\n",
    "    \"\"\"Reads ranking data from a file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path of the file to read.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing the ranking data.\n",
    "    \"\"\"\n",
    "    system_ranking = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            qid, doc_id = parts[0], str(int(parts[2]))\n",
    "            if qid not in system_ranking:\n",
    "                system_ranking[qid] = []\n",
    "            system_ranking[qid].append(doc_id)\n",
    "        \n",
    "    return system_ranking\n",
    "\n",
    "# read_data_ground_true: Reads ground truth data from a file and organizes it into a dictionary\n",
    "def read_data_ground_true(file_path, keys):\n",
    "    \"\"\"Reads ground truth data from a file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path of the file to read.\n",
    "        keys (list): List of query IDs to consider.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing the ground truth data.\n",
    "    \"\"\"\n",
    "    ground_true = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            qid, doc_id, relevance = parts[0], str(int(parts[2])), int(parts[3])\n",
    "            if qid not in keys:\n",
    "                continue\n",
    "            if qid not in ground_true:\n",
    "                ground_true[qid] = {}\n",
    "            ground_true[qid][doc_id] = relevance\n",
    "    return ground_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Mean Average Precision (MAP)\n",
    "def calculate_map(system_ranking, recall_bases):\n",
    "    map_scores = []\n",
    "\n",
    "    for query in system_ranking:\n",
    "        num = 0\n",
    "        count = 0\n",
    "        relevants = 1\n",
    "\n",
    "        for element in system_ranking[query]:\n",
    "            if count != 10:\n",
    "                if element[1] > 0:\n",
    "                    num += relevants / (count + 1)\n",
    "                    relevants += 1\n",
    "                count += 1\n",
    "        \n",
    "        rb = recall_bases[query]\n",
    "\n",
    "        ap = num / rb\n",
    "        map_scores.append(ap)\n",
    "        print(f\"AP {query}: {ap}\")\n",
    "\n",
    "    map_avg = sum(map_scores) / len(map_scores)\n",
    "    print(f\"MAP: {map_avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Mean Reciprocal Rank (MRR)\n",
    "def calculate_mrr(system_ranking):\n",
    "    mrr_scores = []\n",
    "    for query in system_ranking:\n",
    "        count = 0\n",
    "\n",
    "        for element in system_ranking[query]:\n",
    "            if element[1] > 0:\n",
    "                rr = 1 / (count + 1)\n",
    "                mrr_scores.append(rr)\n",
    "                print(f\"RR {query}: {rr}\")\n",
    "                break\n",
    "            if count == 9:\n",
    "                mrr_scores.append(0)\n",
    "                print(f\"RR {query}: 0\")\n",
    "                break\n",
    "            count += 1\n",
    "        \n",
    "    mrr_avg = sum(mrr_scores) / len(mrr_scores)\n",
    "    print(f\"MRR: {mrr_avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dcg: Calculates the Discounted Cumulative Gain\n",
    "def dcg(relevance, k) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the Discounted Cumulative Gain (DCG).\n",
    "\n",
    "    Args:\n",
    "        relevance (list): List of relevance scores.\n",
    "        k (int): The depth of ranking to consider.\n",
    "\n",
    "    Returns:\n",
    "        float: The calculated DCG.\n",
    "    \"\"\"\n",
    "    return relevance[0] + sum([relevance[i] / math.log(i + 1, 2) for i in range(1, min(k, len(relevance)))])\n",
    "\n",
    "# ndcg: Calculates the Normalized Discounted Cumulative Gain\n",
    "def ndcg(system_ranking, ground_true, k):\n",
    "    \"\"\"\n",
    "    Calculates the Normalized Discounted Cumulative Gain (nDCG).\n",
    "\n",
    "    Args:\n",
    "        system_ranking (list): List of ranked items.\n",
    "        ground_true (dict): Ground truth relevance scores.\n",
    "        k (int): The depth of ranking to consider.\n",
    "\n",
    "    Returns:\n",
    "        float: The calculated nDCG.\n",
    "    \"\"\"\n",
    "    # Get relevance scores for items in the system ranking\n",
    "    relevances = [ground_true.get(rank, 0) for rank in system_ranking]\n",
    "\n",
    "    # Get ideal relevance scores\n",
    "    base_ideal = [i for i in ground_true.values()]\n",
    "\n",
    "    # Calculate and return nDCG\n",
    "    return dcg(relevances, k) / dcg(base_ideal, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading\n",
    "\n",
    "# Loading recall data from the specified files\n",
    "recall_base_file = \"./qrel_file/id_counts.txt\"\n",
    "ground_true_file=\"./qrel_file/ordered-qrel-msmarco-2019.txt\"\n",
    "\n",
    "# Using reading functions to load the data\n",
    "recall_bases = read_data_recall_base(recall_base_file)\n",
    "\n",
    "# Using reading functions to load the data\n",
    "recall_bases = read_data_recall_base(recall_base_file)\n",
    "folder_path = \"relevance_file\"\n",
    "ndcg_path = \"NDCG_file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and write nDCG scores to files\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        path = os.path.join(folder_path, filename)\n",
    "        path_ndcg = os.path.join(ndcg_path, filename)\n",
    "        \n",
    "        system_ranking = read_data_2(path)\n",
    "        ground_true = read_data_ground_true(ground_true_file, system_ranking.keys())\n",
    "\n",
    "        with open(path_ndcg, 'w') as file:\n",
    "            for qid in system_ranking.keys():\n",
    "                file.write(qid + \"\\t\" + str(ndcg(system_ranking[qid], ground_true[qid], 10)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print MAP for each ranking file\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        system_ranking = read_data(path)\n",
    "        calculate_map(system_ranking, recall_bases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print MRR for each ranking file\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        system_ranking = read_data(path)\n",
    "        calculate_mrr(system_ranking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAP: 0.0890959203131625 daat bm25\n",
    "\n",
    "MAP: 0.0826614187414304 daat tfidf\n",
    "\n",
    "MAP: 0.08909592031316252 dynamicpruning bm25\n",
    "\n",
    "MAP: 0.08266141874143042 dynamicpruning tfidf\n",
    "\n",
    "\n",
    "MRR: 0.712015503875969 daat bm25\n",
    "\n",
    "MRR: 0.6866925064599484 daat tfidf\n",
    "\n",
    "MRR: 0.7120155038759689 dynamicpruning bm25\n",
    "\n",
    "MRR: 0.6866925064599484 dynamicpruning tfidf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MIRCV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
